{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Env setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install langchain : `pip install langchain`\n",
    "\n",
    "Using OpenAI API as LLM provider : `pip install openai`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set environment variables specific to the LLM provider. Note: I have created an Open API key and stored in the environment variable `OPENAI_API_KEY`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "assert os.environ[\"OPENAI_API_KEY\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bright Socks Co.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"product\"],\n",
    "               template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nRainbow Toes Socks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainbow Socks Co.\n"
     ]
    }
   ],
   "source": [
    "# Using a Chat model in LLMChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"What is a good name for a company that makes {product}?\",\n",
    "            input_variables=[\"product\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "print(chain.run(\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a SequntialChain to create a company name for a product \n",
    "# and then create a catchphrase for a product\n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\"],\n",
    "    template=\"Write a catchphrase for the following company: {company_name}\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRainbow Threads\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "\"Make a statement in style with Rainbow Threads!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\"Make a statement in style with Rainbow Threads!\"\n"
     ]
    }
   ],
   "source": [
    "# combine two LLMChains\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two],\n",
    "                                      verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"colorful socks\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomChain with the Chain class\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Colorful Toes Socks.\n",
      "\n",
      "\"Put a Colorful Spin on Your Step!\"\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good slogan for a company that makes {product}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"colorful socks\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Question Answering with Sources\n",
    "\n",
    "Reference:\n",
    "- https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text\n",
    "with open('state_of_the_union.txt') as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks using splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunks = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the difference between `split_text` & `split_documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# Prerequisite: chromadb module is required. install `pip install chromadb`\n",
    "# Create a Chroma vectorstore from a raw documents.\n",
    "# If a persist_directory is specified, the collection will be persisted there.\n",
    "# Otherwise, the data will be ephemeral in-memory.\n",
    "\n",
    "# also including the source as part of metadata \n",
    "docsearch = Chroma.from_texts(texts=chunks, embedding=embeddings,\n",
    "                  metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(chunks))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question-answering with sources over an index\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\n',\n",
       " 'sources': '31-pl'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"question\": \"What did the president say about Justice Breyer\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the text about Justice Breyer to validate\n",
    "\n",
    "> Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' The president said \"Justice Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"\\n',\n",
       " 'sources': '31-pl'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify a different chain type to load and use (tiktoekn package is a prerequsite)\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(temperature=0), \n",
    "    chain_type=\"map_reduce\", \n",
    "    retriever=docsearch.as_retriever()\n",
    ")\n",
    "\n",
    "chain({\"question\": \"What did the president say about Justice Breyer\"}, \n",
    "      return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is much better than the `stuff` chain type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `combine_documents_chain` param to control the parameters passed to RetrievalQAWithSourcesChain\n",
    "\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = load_qa_with_sources_chain(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "qa = RetrievalQAWithSourcesChain(\n",
    "    combine_documents_chain=qa_chain,\n",
    "    retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\n',\n",
       " 'sources': '31-pl'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"question\": \"What did the president say about Justice Breyer\"}, return_only_outputs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Prompts\n",
    "\n",
    "Use our own prompts with the chain and get the response in Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '\\nIl Presidente non ha fatto alcun riferimento diretto a Justice Breyer nel discorso.\\n',\n",
       " 'sources': '31-pl, 32-pl, 21-pl, 34-pl'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "Respond in Italian.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER IN ITALIAN:\"\"\"\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "qa_chain = load_qa_with_sources_chain(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=PROMPT\n",
    ")\n",
    "qa = RetrievalQAWithSourcesChain(\n",
    "    combine_documents_chain=qa_chain,\n",
    "    retriever=docsearch.as_retriever())\n",
    "\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "qa({\"input_documents\": docsearch.as_retriever().get_relevant_documents(query),\n",
    "    \"question\": query }, \n",
    "   return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for MapReduceDocumentsChain\nreduce_intermediate_steps\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# return intermediate steps for `map_reduce` chain type\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chain \u001b[39m=\u001b[39m load_qa_with_sources_chain(\n\u001b[1;32m      3\u001b[0m     llm\u001b[39m=\u001b[39;49mOpenAI(batch_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m),\n\u001b[1;32m      4\u001b[0m     chain_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmap_reduce\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     reduce_intermediate_steps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chat-langchain/lib/python3.9/site-packages/langchain/chains/qa_with_sources/loading.py:171\u001b[0m, in \u001b[0;36mload_qa_with_sources_chain\u001b[0;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unsupported chain type: \u001b[39m\u001b[39m{\u001b[39;00mchain_type\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould be one of \u001b[39m\u001b[39m{\u001b[39;00mloader_mapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m _func: LoadingCallable \u001b[39m=\u001b[39m loader_mapping[chain_type]\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m _func(llm, verbose\u001b[39m=\u001b[39;49mverbose, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/chat-langchain/lib/python3.9/site-packages/langchain/chains/qa_with_sources/loading.py:106\u001b[0m, in \u001b[0;36m_load_map_reduce_chain\u001b[0;34m(llm, question_prompt, combine_prompt, document_prompt, combine_document_variable_name, map_reduce_document_variable_name, collapse_prompt, reduce_llm, collapse_llm, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     _collapse_llm \u001b[39m=\u001b[39m collapse_llm \u001b[39mor\u001b[39;00m llm\n\u001b[1;32m     97\u001b[0m     collapse_chain \u001b[39m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m     98\u001b[0m         llm_chain\u001b[39m=\u001b[39mLLMChain(\n\u001b[1;32m     99\u001b[0m             llm\u001b[39m=\u001b[39m_collapse_llm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m         document_prompt\u001b[39m=\u001b[39mdocument_prompt,\n\u001b[1;32m    105\u001b[0m     )\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m MapReduceDocumentsChain(\n\u001b[1;32m    107\u001b[0m     llm_chain\u001b[39m=\u001b[39;49mmap_chain,\n\u001b[1;32m    108\u001b[0m     combine_document_chain\u001b[39m=\u001b[39;49mcombine_document_chain,\n\u001b[1;32m    109\u001b[0m     document_variable_name\u001b[39m=\u001b[39;49mmap_reduce_document_variable_name,\n\u001b[1;32m    110\u001b[0m     collapse_document_chain\u001b[39m=\u001b[39;49mcollapse_chain,\n\u001b[1;32m    111\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    112\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chat-langchain/lib/python3.9/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for MapReduceDocumentsChain\nreduce_intermediate_steps\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "# return intermediate steps for `map_reduce` chain type\n",
    "chain = load_qa_with_sources_chain(\n",
    "    llm=OpenAI(batch_size=5, temperature=0),\n",
    "    chain_type=\"map_reduce\",\n",
    "    reduce_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
